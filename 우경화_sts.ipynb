{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "우경화_sts_(lr=5e-5).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPy6B3nTBp345mt73ofM/Vs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "951815910dd849ada6077dca25e7fd0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a22f91fd34c843638cfa6d00af209814",
              "IPY_MODEL_9abc745d08d144528846c628b3841de0",
              "IPY_MODEL_8f19f6de52d74c9192d884a708effa60"
            ],
            "layout": "IPY_MODEL_c95e6e49299d4bd2b45f1968202e5226"
          }
        },
        "a22f91fd34c843638cfa6d00af209814": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e498d30ac94470791304361b44ef5eb",
            "placeholder": "​",
            "style": "IPY_MODEL_f29649f447fe46739c402b62b3d94ba9",
            "value": "100%"
          }
        },
        "9abc745d08d144528846c628b3841de0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7a3fc0c97d94f629a9bd58c38eca428",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_741da1d4244d48bd88d96f561de5cc88",
            "value": 2
          }
        },
        "8f19f6de52d74c9192d884a708effa60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50a95e4101ee443d9832bae33e12d0b5",
            "placeholder": "​",
            "style": "IPY_MODEL_0bce01c5edb14aa2b89ed6fa70e3d64b",
            "value": " 2/2 [00:00&lt;00:00,  5.32ba/s]"
          }
        },
        "c95e6e49299d4bd2b45f1968202e5226": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e498d30ac94470791304361b44ef5eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f29649f447fe46739c402b62b3d94ba9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7a3fc0c97d94f629a9bd58c38eca428": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "741da1d4244d48bd88d96f561de5cc88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "50a95e4101ee443d9832bae33e12d0b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bce01c5edb14aa2b89ed6fa70e3d64b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ttogle918/NLU_3-/blob/main/%EC%9A%B0%EA%B2%BD%ED%99%94_sts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NLU - 문장 유사도 계산 (STS)**"
      ],
      "metadata": {
        "id": "DimWDs7A8PS6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **과제 목표**\n",
        "\n",
        "- 두 개의 한국어 문장을 입력받아 두 문장의 의미적 유사도를 출력\n",
        "- regression task (0 ≤ target ≤ 5)"
      ],
      "metadata": {
        "id": "FJ3Qkc_m8PKE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **학습 데이터셋(Dataset description)**\n",
        "\n",
        "- [KLUE-STS](https://klue-benchmark.com/tasks/67/data/download)\n",
        "  - AIRBNB (에어비앤비 리뷰)\n",
        "  - policy (정책 뉴스)\n",
        "  - paraKOQC (스마트홈 쿼리)\n",
        "\n",
        "  ![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "fmJ0IRR98PCU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **과제 결과물**\n",
        "- 학습된 모델 (모델 자유 선택) (train set만 사용해 학습)\n",
        "- 학습 방식 보고서\n",
        " - 어떤 모델을 선택했나\n",
        " - 어떻게 파라미터를 튜닝했나\n",
        " - 어떤 훈련 과정을 거쳤는가\n",
        "- dev set score (F1)\n",
        "- 문장 유사도를 출력하는 API (프레임워크 자유 선택)"
      ],
      "metadata": {
        "id": "Bxw10bz08O7s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **설치 및 설정(Set up)**"
      ],
      "metadata": {
        "id": "gJR3pTML8crf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9vsMuRZbtG_"
      },
      "outputs": [],
      "source": [
        "# Hugging Face의 트랜스포머 모델 설치\n",
        "!pip install transformers\n",
        "!pip install sentence_transformers datasets\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tensorboard 패키지 설치\n",
        "!pip install jupyter-tensorboard"
      ],
      "metadata": {
        "id": "0mYjuV7f2jWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 라이브러리\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import json\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "import math\n",
        "import logging\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.optim import AdamW\n",
        "from datasets import load_dataset, load_metric\n",
        "from sentence_transformers import SentenceTransformer, models, LoggingHandler, losses, util\n",
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "from sentence_transformers.readers import InputExample\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
        "from transformers import get_linear_schedule_with_warmup"
      ],
      "metadata": {
        "id": "7cgRTDfN88an"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# seed\n",
        "seed = 7777\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# device type 확인\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"# available GPUs : {torch.cuda.device_count()}\")\n",
        "    print(f\"GPU name : {torch.cuda.get_device_name()}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "P3Hxm1n4_iz8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52b71c8c-375a-4840-845b-ddd6ecfb84c9"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# available GPUs : 1\n",
            "GPU name : Tesla T4\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# logger\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s - %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        "    handlers=[LoggingHandler()],\n",
        ")"
      ],
      "metadata": {
        "id": "SCrukh84Boms"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "3Db1jEOa8RHG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09be59de-6206-47a2-9f69-a40427080704"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **데이터셋 불러오기(Load Dataset)**\n",
        "- STS 데이터는 두 개의 문장 쌍과 이 두 문장 사이의 유사도 점수로 구성됨.\n",
        "- STS데이터를 학습하여 문장과 문장간 서로 얼마나 유사한지를 예측."
      ],
      "metadata": {
        "id": "63TSIsF6BqsW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KLUE-STS 데이터셋\n",
        "- [KLUE-STS](https://klue-benchmark.com/tasks/67/data/download)"
      ],
      "metadata": {
        "id": "xAgHi2AoIn9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KLUE-STS 데이터셋 다운로드\n",
        "#dataset = load_dataset('klue', 'sts')\n",
        "klue_sts_train = load_dataset(\"klue\", \"sts\", split='train[:90%]')\n",
        "klue_sts_valid = load_dataset(\"klue\", \"sts\", split='train[-10%:]') # train의 10%를 validation set으로 사용\n",
        "klue_sts_test = load_dataset(\"klue\", \"sts\", split='validation')"
      ],
      "metadata": {
        "id": "EDy1kxlXBrey",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65e1b54d-9e99-491b-f02b-783fb98ffcb3"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-06-09 10:36:55 - Reusing dataset klue (/root/.cache/huggingface/datasets/klue/sts/1.0.0/e0fc3bc3de3eb03be2c92d72fd04a60ecc71903f821619cb28ca0e1e29e4233e)\n",
            "2022-06-09 10:36:56 - Reusing dataset klue (/root/.cache/huggingface/datasets/klue/sts/1.0.0/e0fc3bc3de3eb03be2c92d72fd04a60ecc71903f821619cb28ca0e1e29e4233e)\n",
            "2022-06-09 10:36:56 - Reusing dataset klue (/root/.cache/huggingface/datasets/klue/sts/1.0.0/e0fc3bc3de3eb03be2c92d72fd04a60ecc71903f821619cb28ca0e1e29e4233e)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'shape : train {klue_sts_train.shape}, valid {klue_sts_valid.shape}, test {klue_sts_test.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "252btsq4BD2d",
        "outputId": "a5629541-7eca-4c7f-ace5-992c6d46cc14"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape : train (10501, 5), valid (1167, 5), test (519, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset 데이터셋 중 train 첫째 줄 내용\n",
        "klue_sts_train[0]"
      ],
      "metadata": {
        "id": "r3JYJ5E3C3Sb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a996370d-a643-4dfb-ff2f-a7dfb6b3b79b"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'guid': 'klue-sts-v1_train_00000',\n",
              " 'labels': {'binary-label': 1, 'label': 3.7, 'real-label': 3.714285714285714},\n",
              " 'sentence1': '숙소 위치는 찾기 쉽고 일반적인 한국의 반지하 숙소입니다.',\n",
              " 'sentence2': '숙박시설의 위치는 쉽게 찾을 수 있고 한국의 대표적인 반지하 숙박시설입니다.',\n",
              " 'source': 'airbnb-rtt'}"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset 데이터셋 중 validation 첫째 줄 내용\n",
        "klue_sts_valid[0]"
      ],
      "metadata": {
        "id": "M20zhK3bC4wS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15e6f2ca-3f94-46db-cbda-265e382048f9"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'guid': 'klue-sts-v1_train_10501',\n",
              " 'labels': {'binary-label': 0, 'label': 0.0, 'real-label': 0.0},\n",
              " 'sentence1': '지난해 대비 TV 등 3개 품목이 추가됐다.',\n",
              " 'sentence2': '특히 재활용산업육성자금은 전년 대비 350억 원이 증액됐다.',\n",
              " 'source': 'policy-sampled'}"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **데이터 전처리(Data Preprocessing)**"
      ],
      "metadata": {
        "id": "Q83KGMO1C7tS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 파라미터 정의 \n",
        "task = \"sts\"\n",
        "model_checkpoint = \"klue/roberta-base\"\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "QtirRVyY3zfh"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# klue/roberta-base tokenizer 불러오기 \n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zy1tT8TY3vWS",
        "outputId": "49b3923f-0dd9-4528-e89a-87a1d3b97805"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading file https://huggingface.co/klue/roberta-base/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/e8441a174492958462b6b16b6db8f1e7253cd149ca779522cadd812d55091b89.d1b86bed49516351c7bb29b19d7e7be2ab53b931bcb1f9b2aacfb71f2124d25a\n",
            "loading file https://huggingface.co/klue/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/233a5b2c17873a8477b62dd92a02092a9937759e924a5f22b111becebb8aba5e.44c30ade4958fcfd446e66025e10a5b380cdd0bbe9b3fb7a794f357e7f0f34c2\n",
            "loading file https://huggingface.co/klue/roberta-base/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/klue/roberta-base/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/9d0c87e44b00acfbfbae931b2e4068eb6311a0c3e71e23e5400bdf57cab4bfbf.70c17d6e4d492c8f24f5bb97ab56c7f272e947112c6faf9dd846da42ba13eb23\n",
            "loading file https://huggingface.co/klue/roberta-base/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/9a9f77abeddd1bbd8de28608e78dd3604287ad91abd4796cd25ad936715b7640.5b0ba083b234382bb4c99ee0c9f4fca4cadaa053dd17c32dabfe0de2f629af1f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# real-label을 label로 변경 \n",
        "klue_sts_train = klue_sts_train.flatten()\n",
        "klue_sts_train = klue_sts_train.rename_column('labels.real-label','label')\n",
        "klue_sts_valid = klue_sts_valid.flatten()\n",
        "klue_sts_valid = klue_sts_valid.rename_column('labels.real-label','label')\n",
        "klue_sts_test = klue_sts_test.flatten()\n",
        "klue_sts_test = klue_sts_test.rename_column('labels.real-label','label')"
      ],
      "metadata": {
        "id": "QHOaCpZ83vRf"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 입력으로 만들어 주는 전처리 함수 \n",
        "# 'truncation=True'으로 'max_length=512' 길이 넘는 것들은 잘라버림\n",
        "# 길이가 부족한 문장은 'padding=True'로 길이 맞춰주기 \n",
        "def preprocess_function(dataset):\n",
        "   return tokenizer(dataset['sentence1'], dataset['sentence2'], truncation=True, max_length=512, padding=True)"
      ],
      "metadata": {
        "id": "qb5niQ5W3vJ0"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 함수 각 데이터셋에 적용 \n",
        "encoded_train = klue_sts_train.map(preprocess_function, batched=True)\n",
        "encoded_valid = klue_sts_valid.map(preprocess_function, batched=True)\n",
        "encoded_test = klue_sts_test.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "951815910dd849ada6077dca25e7fd0e",
            "a22f91fd34c843638cfa6d00af209814",
            "9abc745d08d144528846c628b3841de0",
            "8f19f6de52d74c9192d884a708effa60",
            "c95e6e49299d4bd2b45f1968202e5226",
            "8e498d30ac94470791304361b44ef5eb",
            "f29649f447fe46739c402b62b3d94ba9",
            "c7a3fc0c97d94f629a9bd58c38eca428",
            "741da1d4244d48bd88d96f561de5cc88",
            "50a95e4101ee443d9832bae33e12d0b5",
            "0bce01c5edb14aa2b89ed6fa70e3d64b"
          ]
        },
        "id": "64pYk7xM3vFM",
        "outputId": "4d8d29fb-0b7d-4fdb-c89b-796d41aff9ac"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-06-09 10:36:58 - Loading cached processed dataset at /root/.cache/huggingface/datasets/klue/sts/1.0.0/e0fc3bc3de3eb03be2c92d72fd04a60ecc71903f821619cb28ca0e1e29e4233e/cache-c2ce9cfdce0b5349.arrow\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "951815910dd849ada6077dca25e7fd0e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-06-09 10:36:59 - Loading cached processed dataset at /root/.cache/huggingface/datasets/klue/sts/1.0.0/e0fc3bc3de3eb03be2c92d72fd04a60ecc71903f821619cb28ca0e1e29e4233e/cache-09a4cfac36e05d79.arrow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## **모델 훈련 & 모델 평가(Model Training & Model Evaluation)**"
      ],
      "metadata": {
        "id": "udb5fO6uxlnf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###**Model : klue/roberta-base**\n",
        "batch_size : 32"
      ],
      "metadata": {
        "id": "q8i7H7WP-MZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_labels = 1\n",
        "\n",
        "# model : klue/roberta-base 사용\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5g4OcQzh8Oy",
        "outputId": "adbdf021-1085-4949-e6d4-347e10e34727"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file https://huggingface.co/klue/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a96469ca2a238496d435a0e9e202f261119c146a0326444b6d68ae1adc35e04f.85b0b02ba2a483f3adb8a60ab70dbd875768fcd5e6cdb21a593c6e02fdffac3a\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"klue/roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BertTokenizer\",\n",
            "  \"transformers_version\": \"4.19.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/klue/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/b204e0dc0a3b8fd45b35e7fcefd97c5f839b86c14aea510f1eb38fb8469e23d8.57d3cd0dfa80e5a249a776870dc87b6da993900685a271086750174009115320\n",
            "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 텐서보드 dir 정의 \n",
        "logdir_path = \"/content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/logs\" # log 파일을 저장할 경로를 지정\n",
        "logdir_path = os.path.join(logdir_path, \"sts_log\") # 이번 프로젝트 로그를 저장할 파일 경로 정의\n",
        "writer = SummaryWriter(logdir_path) "
      ],
      "metadata": {
        "id": "V2DYlh1i8Bgb"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# baseline model\n",
        "# batch_size= 32, 나머지는 default\n",
        "\n",
        "output_dir = os.path.join(\"/content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp\", \"sts\")\n",
        "\n",
        "args = TrainingArguments(\n",
        "    # checkpoint\n",
        "    output_dir=output_dir,\n",
        "\n",
        "    # Model Save & Load\n",
        "    save_strategy = \"epoch\", # 각 epoch 마지막에 저장 \n",
        "    load_best_model_at_end=True, # train 종료시 best model 로드할지 여부\n",
        "\n",
        "    # Dataset\n",
        "    num_train_epochs=4,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    \n",
        "    # Evaluation \n",
        "    evaluation_strategy = \"epoch\",# 각 epoch 마지막에 평가\n",
        "\n",
        "    # Randomness\n",
        "    seed=42\n",
        ")"
      ],
      "metadata": {
        "id": "YPXa2hl42ABI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68334e49-5bf1-4883-a36f-6257768878c6"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# f1과 pearson 메트릭 불러오기 \n",
        "metric_f1 = load_metric(\"f1\") \n",
        "metric_pearson = load_metric(\"pearsonr\") "
      ],
      "metadata": {
        "id": "4Utl51moiD8A"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# metric: pearsonr, F1 score\n",
        "metric_name = \"pearsonr\"\n",
        "metric_pearsonr = load_metric(metric_name) # peason r\n",
        "metric_name2 = \"f1\"\n",
        "metric_f1 = load_metric(metric_name2) # f1"
      ],
      "metadata": {
        "id": "PxutJNk3ILQU"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 출력시 메트릭 정의 하는 함수 \n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = predictions[:, 0]\n",
        "\n",
        "    # 3.0 이상 -> '1' , 3.0 미만 -> '0'으로 변환\n",
        "    predict_f1 = [1 if i>=3.0 else 0 for i in predictions] \n",
        "    labels_f1 = [1 if i>=3.0 else 0 for i in labels]\n",
        "\n",
        "    # peason r 계산\n",
        "    peason_r = metric_pearson.compute(predictions=predictions, references=labels)\n",
        "\n",
        "    # f1\n",
        "    f1 = metric_f1.compute(predictions=predict_f1, references=labels_f1)\n",
        "\n",
        "    return {'pearsonr' : peason_r, 'f1' : f1}"
      ],
      "metadata": {
        "id": "LO59BdF-0wXT"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jhJB1NJOv1y"
      },
      "source": [
        "# trainer \n",
        "model_trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset = encoded_train,\n",
        "    eval_dataset = encoded_valid,\n",
        "    tokenizer = tokenizer,\n",
        "    compute_metrics = compute_metrics,\n",
        "    callbacks = [EarlyStoppingCallback(early_stopping_patience = 1)]\n",
        ")"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_trainer.train()"
      ],
      "metadata": {
        "id": "56IzEpmk8cuW",
        "outputId": "4f3cf1e2-f856-4582-cfef-f6c4e6dbc809",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, source, labels.label, labels.binary-label, sentence2, guid. If sentence1, source, labels.label, labels.binary-label, sentence2, guid are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 10501\n",
            "  Num Epochs = 4\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1316\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='987' max='1316' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 987/1316 11:55 < 03:58, 1.38 it/s, Epoch 3/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Pearsonr</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.292051</td>\n",
              "      <td>{'pearsonr': 0.9707647775601107}</td>\n",
              "      <td>{'f1': 0.9508771929824562}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.388800</td>\n",
              "      <td>0.172854</td>\n",
              "      <td>{'pearsonr': 0.9746129190102994}</td>\n",
              "      <td>{'f1': 0.9597855227882036}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.388800</td>\n",
              "      <td>0.191283</td>\n",
              "      <td>{'pearsonr': 0.9785844701147102}</td>\n",
              "      <td>{'f1': 0.9590747330960854}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, source, labels.label, labels.binary-label, sentence2, guid. If sentence1, source, labels.label, labels.binary-label, sentence2, guid are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 32\n",
            "Trainer is attempting to log a value of \"{'pearsonr': 0.9707647775601107}\" of type <class 'dict'> for key \"eval/pearsonr\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"{'f1': 0.9508771929824562}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-329\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-329/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-329/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-329/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-329/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, source, labels.label, labels.binary-label, sentence2, guid. If sentence1, source, labels.label, labels.binary-label, sentence2, guid are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 32\n",
            "Trainer is attempting to log a value of \"{'pearsonr': 0.9746129190102994}\" of type <class 'dict'> for key \"eval/pearsonr\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"{'f1': 0.9597855227882036}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-658\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-658/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-658/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-658/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-658/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, source, labels.label, labels.binary-label, sentence2, guid. If sentence1, source, labels.label, labels.binary-label, sentence2, guid are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 32\n",
            "Trainer is attempting to log a value of \"{'pearsonr': 0.9785844701147102}\" of type <class 'dict'> for key \"eval/pearsonr\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"{'f1': 0.9590747330960854}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-987\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-987/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-987/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-987/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-987/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-658 (score: 0.17285382747650146).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=987, training_loss=0.25219582400181856, metrics={'train_runtime': 716.1926, 'train_samples_per_second': 58.649, 'train_steps_per_second': 1.837, 'total_flos': 2038396057643070.0, 'train_loss': 0.25219582400181856, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "aaaf34ee-1341-411e-cad9-bfbf7f84a01d",
        "id": "inF3Dt40muCT"
      },
      "source": [
        "# baseline score\n",
        "model_trainer.evaluate()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, source, labels.label, labels.binary-label, sentence2, guid. If sentence1, source, labels.label, labels.binary-label, sentence2, guid are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='37' max='37' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [37/37 00:07]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Trainer is attempting to log a value of \"{'pearsonr': 0.9746129190102994}\" of type <class 'dict'> for key \"eval/pearsonr\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"{'f1': 0.9597855227882036}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'epoch': 3.0,\n",
              " 'eval_f1': {'f1': 0.9597855227882036},\n",
              " 'eval_loss': 0.17285382747650146,\n",
              " 'eval_pearsonr': {'pearsonr': 0.9746129190102994},\n",
              " 'eval_runtime': 7.3814,\n",
              " 'eval_samples_per_second': 158.101,\n",
              " 'eval_steps_per_second': 5.013}"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test set으로 prediction \n",
        "predictions = model_trainer.predict(encoded_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "9gElrs4R5Mq5",
        "outputId": "80a23ae4-2d74-411d-c1e4-38fd82bc087f"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, source, labels.label, labels.binary-label, sentence2, guid. If sentence1, source, labels.label, labels.binary-label, sentence2, guid are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Prediction *****\n",
            "  Num examples = 519\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='54' max='37' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [37/37 00:10]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test set score \n",
        "predictions.metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9536c84-c8f9-48e0-e76d-739356c398df",
        "id": "4-UtlDq5m3am"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test_f1': {'f1': 0.8495934959349595},\n",
              " 'test_loss': 0.41511061787605286,\n",
              " 'test_pearsonr': {'pearsonr': 0.9092805311609943},\n",
              " 'test_runtime': 3.3949,\n",
              " 'test_samples_per_second': 152.875,\n",
              " 'test_steps_per_second': 5.007}"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "###**Model : klue/roberta-base**\n",
        "batch_size : 16"
      ],
      "metadata": {
        "id": "6GISTMSH_LSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_size= 16\n",
        "\n",
        "output_dir = os.path.join(\"/content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp\", \"sts\")\n",
        "\n",
        "batch_args = TrainingArguments(\n",
        "    # checkpoint\n",
        "    output_dir = output_dir,\n",
        "\n",
        "    # Model Save & Load\n",
        "    save_strategy = \"epoch\", # 각 epoch 마지막에 저장 \n",
        "    load_best_model_at_end = True, # train 종료시 best model 로드할지 여부\n",
        "\n",
        "    # Dataset\n",
        "    num_train_epochs = 4,\n",
        "    per_device_train_batch_size = 16,\n",
        "    per_device_eval_batch_size = 16,\n",
        "    \n",
        "    # Optimizer (AdamW)\n",
        "    learning_rate =  5e-5,\n",
        "    warmup_steps = 200, \n",
        "    weight_decay = 0.01,\n",
        "\n",
        "    # Evaluation \n",
        "    evaluation_strategy = \"epoch\",# 각 epoch 마지막에 평가\n",
        "\n",
        "    # Randomness\n",
        "    seed = 42\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Av69nKiX_0ts",
        "outputId": "5b2d45a1-dee7-4459-b55b-a725eeba5638"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer 및 scheduler 설정: 16\n",
        "optimizer = AdamW(\n",
        "    model.parameters(),\n",
        "    lr = 5e-5,\n",
        "    eps = 1e-8\n",
        ")\n",
        "\n",
        "batch_size = 16\n",
        "total_steps = round(len(encoded_train.shard(index=1, num_shards=10)) / batch_size) * 5\n",
        "\n",
        "# lr 선형으로 감소시키는 스케줄러\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)"
      ],
      "metadata": {
        "id": "xZRUEUaVYyzy"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_size : 16\n",
        "model_trainer = Trainer(\n",
        "    model,\n",
        "    args = batch_args,\n",
        "    train_dataset = encoded_train ,\n",
        "    eval_dataset = encoded_valid,\n",
        "    tokenizer = tokenizer,\n",
        "    compute_metrics = compute_metrics,\n",
        "    optimizers = (optimizer, scheduler),\n",
        "    callbacks = [EarlyStoppingCallback(early_stopping_patience = 1)]\n",
        ")"
      ],
      "metadata": {
        "id": "KX4AGpX3ZKaH"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_trainer.train()"
      ],
      "metadata": {
        "outputId": "7aaa6b38-380a-4a9f-a5a4-9a1064eaee52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "id": "113W49p0_0tw"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, source, labels.label, labels.binary-label, sentence2, guid. If sentence1, source, labels.label, labels.binary-label, sentence2, guid are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running training *****\n",
            "  Num examples = 10501\n",
            "  Num Epochs = 4\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 2628\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1314' max='2628' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1314/2628 08:19 < 08:20, 2.63 it/s, Epoch 2/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Pearsonr</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.120800</td>\n",
              "      <td>0.171174</td>\n",
              "      <td>{'pearsonr': 0.9761494725561197}</td>\n",
              "      <td>{'f1': 0.9566003616636528}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.089300</td>\n",
              "      <td>0.171174</td>\n",
              "      <td>{'pearsonr': 0.9761494725561197}</td>\n",
              "      <td>{'f1': 0.9566003616636528}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, source, labels.label, labels.binary-label, sentence2, guid. If sentence1, source, labels.label, labels.binary-label, sentence2, guid are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 16\n",
            "Trainer is attempting to log a value of \"{'pearsonr': 0.9761494725561197}\" of type <class 'dict'> for key \"eval/pearsonr\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"{'f1': 0.9566003616636528}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-657\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-657/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-657/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-657/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-657/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, source, labels.label, labels.binary-label, sentence2, guid. If sentence1, source, labels.label, labels.binary-label, sentence2, guid are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 16\n",
            "Trainer is attempting to log a value of \"{'pearsonr': 0.9761494725561197}\" of type <class 'dict'> for key \"eval/pearsonr\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"{'f1': 0.9566003616636528}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-1314\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-1314/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-1314/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-1314/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-1314/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-657 (score: 0.1711743026971817).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1314, training_loss=0.10093667191457531, metrics={'train_runtime': 499.7476, 'train_samples_per_second': 84.05, 'train_steps_per_second': 5.259, 'total_flos': 1354533912171084.0, 'train_loss': 0.10093667191457531, 'epoch': 2.0})"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "Ec1OAbd0_0ty",
        "outputId": "30d10f7e-ac70-493e-82c1-b7dd3c4ff9b0"
      },
      "source": [
        "# score\n",
        "model_trainer.evaluate()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, source, labels.label, labels.binary-label, sentence2, guid. If sentence1, source, labels.label, labels.binary-label, sentence2, guid are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='73' max='73' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [73/73 00:07]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Trainer is attempting to log a value of \"{'pearsonr': 0.9761494725561197}\" of type <class 'dict'> for key \"eval/pearsonr\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"{'f1': 0.9566003616636528}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'epoch': 2.0,\n",
              " 'eval_f1': {'f1': 0.9566003616636528},\n",
              " 'eval_loss': 0.1711743026971817,\n",
              " 'eval_pearsonr': {'pearsonr': 0.9761494725561197},\n",
              " 'eval_runtime': 7.8387,\n",
              " 'eval_samples_per_second': 148.876,\n",
              " 'eval_steps_per_second': 9.313}"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test set으로 prediction \n",
        "predictions = model_trainer.predict(encoded_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "48mR_8n7CFf_",
        "outputId": "4fb49ea2-879e-49f2-de2b-2c2002518489"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, source, labels.label, labels.binary-label, sentence2, guid. If sentence1, source, labels.label, labels.binary-label, sentence2, guid are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Prediction *****\n",
            "  Num examples = 519\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='106' max='73' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [73/73 00:11]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test set score \n",
        "predictions.metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jua3t555CItM",
        "outputId": "a862feea-565c-4062-e1dc-ca61d6e48492"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test_f1': {'f1': 0.8601694915254239},\n",
              " 'test_loss': 0.3401721715927124,\n",
              " 'test_pearsonr': {'pearsonr': 0.9233630152614025},\n",
              " 'test_runtime': 3.5131,\n",
              " 'test_samples_per_second': 147.734,\n",
              " 'test_steps_per_second': 9.394}"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "###**Model : klue/roberta-base**\n",
        "batch_size : 32"
      ],
      "metadata": {
        "id": "shwOiR_fBhGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_size= 32\n",
        "\n",
        "output_dir = os.path.join(\"/content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp\", \"sts\")\n",
        "\n",
        "batch_args = TrainingArguments(\n",
        "    # checkpoint\n",
        "    output_dir = output_dir,\n",
        "\n",
        "    # Model Save & Load\n",
        "    save_strategy = \"epoch\", # 각 epoch 마지막에 저장 \n",
        "    load_best_model_at_end = True, # train 종료시 best model 로드할지 여부\n",
        "\n",
        "    # Dataset\n",
        "    num_train_epochs = 4,\n",
        "    per_device_train_batch_size = 32,\n",
        "    per_device_eval_batch_size = 32,\n",
        "    \n",
        "    # Optimizer (AdamW)\n",
        "    learning_rate =  5e-5,\n",
        "    warmup_steps = 200,\n",
        "    weight_decay = 0.01,\n",
        "\n",
        "    # Evaluation \n",
        "    evaluation_strategy = \"epoch\",# 각 epoch 마지막에 평가\n",
        "\n",
        "    # Randomness\n",
        "    seed = 42\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6PERkThBjR7",
        "outputId": "1f15a311-1351-4a3b-cdca-7041cf5dbcff"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer 및 scheduler 설정: 32\n",
        "optimizer = AdamW(\n",
        "    model.parameters(),\n",
        "    lr = 5e-5,\n",
        "    eps = 1e-8\n",
        ")\n",
        "\n",
        "batch_size = 32\n",
        "total_steps = round(len(encoded_train.shard(index=1, num_shards=10)) / batch_size) * 5\n",
        "\n",
        "# lr 선형으로 감소시키는 스케줄러\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)"
      ],
      "metadata": {
        "id": "n8hCziZvBjR9"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_size : 32\n",
        "model_trainer = Trainer(\n",
        "    model,\n",
        "    args = batch_args,\n",
        "    train_dataset = encoded_train ,\n",
        "    eval_dataset = encoded_valid,\n",
        "    tokenizer = tokenizer,\n",
        "    compute_metrics = compute_metrics,\n",
        "    optimizers = (optimizer, scheduler),\n",
        "    callbacks = [EarlyStoppingCallback(early_stopping_patience = 1)]\n",
        ")"
      ],
      "metadata": {
        "id": "Nw6uLAO_BjR9"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_trainer.train()"
      ],
      "metadata": {
        "outputId": "c5faebe9-dc0d-4a70-eff5-5ceeebeab83e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "id": "-5O0vfwcBjR-"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, source, labels.label, labels.binary-label, sentence2, guid. If sentence1, source, labels.label, labels.binary-label, sentence2, guid are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running training *****\n",
            "  Num examples = 10501\n",
            "  Num Epochs = 4\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1316\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='658' max='1316' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 658/1316 07:56 < 07:57, 1.38 it/s, Epoch 2/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Pearsonr</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.159081</td>\n",
              "      <td>{'pearsonr': 0.9778043280043923}</td>\n",
              "      <td>{'f1': 0.9592760180995474}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.086900</td>\n",
              "      <td>0.159081</td>\n",
              "      <td>{'pearsonr': 0.9778043280043923}</td>\n",
              "      <td>{'f1': 0.9592760180995474}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, source, labels.label, labels.binary-label, sentence2, guid. If sentence1, source, labels.label, labels.binary-label, sentence2, guid are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 32\n",
            "Trainer is attempting to log a value of \"{'pearsonr': 0.9778043280043923}\" of type <class 'dict'> for key \"eval/pearsonr\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"{'f1': 0.9592760180995474}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-329\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-329/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-329/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-329/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-329/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, source, labels.label, labels.binary-label, sentence2, guid. If sentence1, source, labels.label, labels.binary-label, sentence2, guid are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 32\n",
            "Trainer is attempting to log a value of \"{'pearsonr': 0.9778043280043923}\" of type <class 'dict'> for key \"eval/pearsonr\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"{'f1': 0.9592760180995474}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-658\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-658/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-658/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-658/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-658/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-329 (score: 0.1590813398361206).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=658, training_loss=0.0819314055167433, metrics={'train_runtime': 476.839, 'train_samples_per_second': 88.088, 'train_steps_per_second': 2.76, 'total_flos': 1359072537125196.0, 'train_loss': 0.0819314055167433, 'epoch': 2.0})"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "iwx3KLiuBjR_",
        "outputId": "1c80fc62-8a58-4eeb-cbcc-43a9b41bc0d1"
      },
      "source": [
        "# score\n",
        "model_trainer.evaluate()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, source, labels.label, labels.binary-label, sentence2, guid. If sentence1, source, labels.label, labels.binary-label, sentence2, guid are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='37' max='37' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [37/37 00:07]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Trainer is attempting to log a value of \"{'pearsonr': 0.9778043280043923}\" of type <class 'dict'> for key \"eval/pearsonr\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"{'f1': 0.9592760180995474}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'epoch': 2.0,\n",
              " 'eval_f1': {'f1': 0.9592760180995474},\n",
              " 'eval_loss': 0.1590813398361206,\n",
              " 'eval_pearsonr': {'pearsonr': 0.9778043280043923},\n",
              " 'eval_runtime': 7.4267,\n",
              " 'eval_samples_per_second': 157.136,\n",
              " 'eval_steps_per_second': 4.982}"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test set으로 prediction \n",
        "predictions = model_trainer.predict(encoded_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "p_ciRH0I82Uc",
        "outputId": "ca8a4787-57fc-4fcf-cceb-e535a9c0ca41"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, source, labels.label, labels.binary-label, sentence2, guid. If sentence1, source, labels.label, labels.binary-label, sentence2, guid are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Prediction *****\n",
            "  Num examples = 519\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='54' max='37' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [37/37 00:10]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test set score \n",
        "predictions.metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Li5QB5d822T",
        "outputId": "4b9a1423-fd02-46b0-df4b-0104daccd152"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test_f1': {'f1': 0.8530020703933748},\n",
              " 'test_loss': 0.36934179067611694,\n",
              " 'test_pearsonr': {'pearsonr': 0.9201903163649622},\n",
              " 'test_runtime': 3.4281,\n",
              " 'test_samples_per_second': 151.394,\n",
              " 'test_steps_per_second': 4.959}"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "###**Model : klue/roberta-base**\n",
        "batch_size : 64"
      ],
      "metadata": {
        "id": "B5bHKMAp_OMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_size= 64\n",
        "\n",
        "output_dir = os.path.join(\"/content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp\", \"sts\")\n",
        "\n",
        "batch_args = TrainingArguments(\n",
        "    # checkpoint\n",
        "    output_dir = output_dir,\n",
        "\n",
        "    # Model Save & Load\n",
        "    save_strategy = \"epoch\", # 각 epoch 마지막에 저장 \n",
        "    load_best_model_at_end = True, # train 종료시 best model 로드할지 여부\n",
        "\n",
        "    # Dataset\n",
        "    num_train_epochs = 4,\n",
        "    per_device_train_batch_size = 64,\n",
        "    per_device_eval_batch_size = 64,\n",
        "    \n",
        "    # Optimizer (AdamW)\n",
        "    learning_rate =  5e-5,\n",
        "    warmup_steps = 200,\n",
        "    weight_decay = 0.01,\n",
        "\n",
        "    # Evaluation \n",
        "    evaluation_strategy = \"epoch\",# 각 epoch 마지막에 평가\n",
        "\n",
        "    # Randomness\n",
        "    seed = 42\n",
        ")"
      ],
      "metadata": {
        "id": "ylHF8J5iBWGg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef025c3e-2595-46b3-c41d-e89132e40257"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer 및 scheduler 설정: 64\n",
        "optimizer = AdamW(\n",
        "    model.parameters(),\n",
        "    lr = 5e-5,\n",
        "    eps = 1e-8\n",
        ")\n",
        "\n",
        "batch_size = 64\n",
        "total_steps = round(len(encoded_train.shard(index=1, num_shards=10)) / batch_size) * 5\n",
        "\n",
        "# lr 선형으로 감소시키는 스케줄러\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)"
      ],
      "metadata": {
        "id": "DhBB28PhBWGi"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_size : 64\n",
        "model_trainer = Trainer(\n",
        "    model,\n",
        "    args = batch_args,\n",
        "    train_dataset = encoded_train ,\n",
        "    eval_dataset = encoded_valid,\n",
        "    tokenizer = tokenizer,\n",
        "    compute_metrics = compute_metrics,\n",
        "    optimizers = (optimizer, scheduler),\n",
        "    callbacks = [EarlyStoppingCallback(early_stopping_patience = 1)]\n",
        ")"
      ],
      "metadata": {
        "id": "1yjfc9B7BWGj"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_trainer.train()"
      ],
      "metadata": {
        "outputId": "359719dd-b743-4962-ed16-57671da5d6f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "id": "GozcahrHBWGk"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, source, labels.label, labels.binary-label, sentence2, guid. If sentence1, source, labels.label, labels.binary-label, sentence2, guid are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running training *****\n",
            "  Num examples = 10501\n",
            "  Num Epochs = 4\n",
            "  Instantaneous batch size per device = 64\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 660\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='330' max='660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [330/660 07:28 < 07:30, 0.73 it/s, Epoch 2/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Pearsonr</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.176367</td>\n",
              "      <td>{'pearsonr': 0.9780379413470249}</td>\n",
              "      <td>{'f1': 0.9607142857142857}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.176367</td>\n",
              "      <td>{'pearsonr': 0.9780379413470249}</td>\n",
              "      <td>{'f1': 0.9607142857142857}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, source, labels.label, labels.binary-label, sentence2, guid. If sentence1, source, labels.label, labels.binary-label, sentence2, guid are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "Trainer is attempting to log a value of \"{'pearsonr': 0.9780379413470249}\" of type <class 'dict'> for key \"eval/pearsonr\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"{'f1': 0.9607142857142857}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-165\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-165/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-165/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-165/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-165/special_tokens_map.json\n",
            "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, source, labels.label, labels.binary-label, sentence2, guid. If sentence1, source, labels.label, labels.binary-label, sentence2, guid are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n",
            "Trainer is attempting to log a value of \"{'pearsonr': 0.9780379413470249}\" of type <class 'dict'> for key \"eval/pearsonr\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"{'f1': 0.9607142857142857}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-330\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-330/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-330/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-330/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-330/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from /content/drive/MyDrive/Colab Notebooks/NLP with 파이토치/nlp/sts/checkpoint-165 (score: 0.17636741697788239).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=330, training_loss=0.08469009977398496, metrics={'train_runtime': 449.6007, 'train_samples_per_second': 93.425, 'train_steps_per_second': 1.468, 'total_flos': 1359664531684428.0, 'train_loss': 0.08469009977398496, 'epoch': 2.0})"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhyLLoEmBWGl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "199223a6-d2e5-431b-a9fe-a832c99deda1"
      },
      "source": [
        "# score\n",
        "model_trainer.evaluate()"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, source, labels.label, labels.binary-label, sentence2, guid. If sentence1, source, labels.label, labels.binary-label, sentence2, guid are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1167\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [19/19 00:06]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Trainer is attempting to log a value of \"{'pearsonr': 0.9780379413470249}\" of type <class 'dict'> for key \"eval/pearsonr\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
            "Trainer is attempting to log a value of \"{'f1': 0.9607142857142857}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'epoch': 2.0,\n",
              " 'eval_f1': {'f1': 0.9607142857142857},\n",
              " 'eval_loss': 0.17636741697788239,\n",
              " 'eval_pearsonr': {'pearsonr': 0.9780379413470249},\n",
              " 'eval_runtime': 7.4178,\n",
              " 'eval_samples_per_second': 157.323,\n",
              " 'eval_steps_per_second': 2.561}"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test set으로 prediction \n",
        "predictions = model_trainer.predict(encoded_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "Zpka1jbgCC0Z",
        "outputId": "e321b1ee-b1d4-4a6c-8451-6f10782e07e1"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, source, labels.label, labels.binary-label, sentence2, guid. If sentence1, source, labels.label, labels.binary-label, sentence2, guid are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Prediction *****\n",
            "  Num examples = 519\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='28' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [19/19 00:10]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test set score \n",
        "predictions.metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awMnSugRCLAg",
        "outputId": "567768b5-9dc6-4746-b1b2-c2e1cd841c5b"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test_f1': {'f1': 0.8553719008264462},\n",
              " 'test_loss': 0.37406018376350403,\n",
              " 'test_pearsonr': {'pearsonr': 0.9221298254042499},\n",
              " 'test_runtime': 3.2815,\n",
              " 'test_samples_per_second': 158.161,\n",
              " 'test_steps_per_second': 2.743}"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "nNgJ2qrOX5lY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model : klue/roberta-base** (가장 성능 좋음)\n",
        "- epochs : 4\n",
        "- batch_size : train 16\n",
        "- learning_rate : 5e-5\n",
        "- weight_decay : 0.01\n",
        "- loss : 0.3401721715927124,\n",
        "- pearson : 0.9233630152614025\n",
        "- f1 : 0.8601694915254239"
      ],
      "metadata": {
        "id": "EuPg1LF4YAKr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model : klue/roberta-base**\n",
        "- epochs : 4\n",
        "- batch_size : train 32\n",
        "- learning_rate : 5e-5\n",
        "- weight_decay : 0.01\n",
        "- loss : 0.36934179067611694\n",
        "- pearson : 0.9201903163649622\n",
        "- f1 : 0.8530020703933748"
      ],
      "metadata": {
        "id": "0IaGvCPhZm-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model : klue/roberta-base**\n",
        "- epochs : 4\n",
        "- batch_size : train 64\n",
        "- learning_rate : 5e-5\n",
        "- weight_decay : 0.01\n",
        "- loss : 0.37406018376350403\n",
        "- pearson 상관 계수 : 0.9221298254042499\n",
        "- f1 : 0.8553719008264462"
      ],
      "metadata": {
        "id": "4Pd1rLGeaGZC"
      }
    }
  ]
}