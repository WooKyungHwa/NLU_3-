{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing_to_model.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ttogle918/NLU_3-/blob/main/preprocessing_to_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 패키지 설치 및 선언"
      ],
      "metadata": {
        "id": "E2-3ecFqV70G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GuyuacIVkYS"
      },
      "outputs": [],
      "source": [
        "!pip install optuna\n",
        "!pip install pytorch-transformers\n",
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim import AdamW\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "from sklearn.metrics import f1_score\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "QkDMuSKGVr1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gpu 연산이 가능하면 'cuda:0', 아니면 'cpu' 출력\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device, torch.cuda.device_count()"
      ],
      "metadata": {
        "id": "OH6A9OTCVs2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForNextSentencePrediction, AutoTokenizer, BertConfig\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup\n",
        "from transformers import get_linear_schedule_with_warmup"
      ],
      "metadata": {
        "id": "72x84dgjVvU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model class ( 1 ~ 4 )"
      ],
      "metadata": {
        "id": "39mTqIo4WWNx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### dataset Tokenizing ( 1, 2번 해당 )\n",
        "\n",
        "CustomDataset"
      ],
      "metadata": {
        "id": "32FFkPl_V-3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset('klue', 'sts')"
      ],
      "metadata": {
        "id": "cZQh74V0WAE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler"
      ],
      "metadata": {
        "id": "NKMH_SQuWPRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataset, appended_data=None, appended_data2=None):\n",
        "        self.sentence1, self.sentence2, self.labels = self.make_dataset(dataset, appended_data, appended_data2)\n",
        "\n",
        "    def make_dataset(self, dataset, appended_data=None, appended_data2=None):\n",
        "        \"\"\"\n",
        "        self.label : dataset의 label의 list\n",
        "        self.input : sentence1, sentence2를 tokenizer한 값을 이어 붙임 \n",
        "        rlabels : # real-label\n",
        "        \"\"\"\n",
        "        sentence1, sentence2, rlabels = [], [], []\n",
        "\n",
        "        for data in dataset :\n",
        "          rlabels.append(data['labels']['real-label'])\n",
        "          sentence1.append(self.cleaning(data['sentence1']))\n",
        "          sentence2.append(self.cleaning(data['sentence2']))\n",
        "\n",
        "        if appended_data is not None :\n",
        "          for data in appended_data :\n",
        "            if data['score'] is None or data['sentence1'] is None or data['sentence2'] is None :\n",
        "              continue \n",
        "            rlabels.append(data['score'])\n",
        "            sentence1.append(self.cleaning(data['sentence1']))\n",
        "            sentence2.append(self.cleaning(data['sentence2']))\n",
        "\n",
        "        if appended_data2 is not None :\n",
        "          for data in appended_data2 :\n",
        "            if data['score'] is None or data['sentence1'] is None or data['sentence2'] is None :\n",
        "              continue\n",
        "            rlabels.append(data['score'])\n",
        "            sentence1.append(self.cleaning(data['sentence1']))\n",
        "            sentence2.append(self.cleaning(data['sentence2']))\n",
        "        return sentence1, sentence2, rlabels\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sentence1[idx], self.sentence2[idx], self.labels[idx]\n",
        "\n",
        "    def cleaning(self, sentence) :\n",
        "        # return re.sub('[^가-힣]','', sentence)    # data preprocessing\n",
        "        return sentence"
      ],
      "metadata": {
        "id": "KJMU8PzoWQwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_fn(batch):\n",
        "    input1_list, input2_list, target_list = [], [], []\n",
        "\n",
        "    for _input1, _input2, _target in batch:\n",
        "        input1_list.append(_input1)\n",
        "        input2_list.append(_input2)\n",
        "        target_list.append(_target)\n",
        "    \n",
        "    tensorized_input = tokenizer(\n",
        "        input1_list, input2_list,\n",
        "        add_special_tokens=True,\n",
        "        padding=\"longest\",  # 배치내 가장 긴 문장을 기준으로 부족한 문장은 [PAD] 토큰을 추가\n",
        "        truncation=True, # max_length를 넘는 문장은 이 후 토큰을 제거함\n",
        "        max_length=512,\n",
        "        return_tensors='pt' # 토크나이즈된 결과 값을 텐서 형태로 반환\n",
        "    )\n",
        "    tensorized_label = torch.tensor(target_list)\n",
        "\n",
        "    return tensorized_input, tensorized_label"
      ],
      "metadata": {
        "id": "cQFY-IEuWSEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dataloader(dataset, tok_model, batch_size, s='train') :\n",
        "  global tokenizer\n",
        "  tokenizer = AutoTokenizer.from_pretrained(tok_model)\n",
        "  if s == 'train' :\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size =batch_size,\n",
        "        sampler = RandomSampler(dataset),\n",
        "        collate_fn = custom_collate_fn\n",
        "    )\n",
        "  else :\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size =batch_size,\n",
        "        sampler = SequentialSampler(dataset),\n",
        "        collate_fn = custom_collate_fn\n",
        "    )\n",
        "  print(f'batch_size : {batch_size}')\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "Na-PTLbPWVXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [1] BERT일 경우\n",
        "\n",
        "BERT일 경우, BertForNextSentencePrediction 클래스로 Pre-trained된 모델을 받아서 Fine-Tuning => 문장의 유사성(STS)과 다음 문장 예측(NSP)은 목적이 다르다. BertModel에 layer를 쌓아 STS를 구하는 것이 낫다.\n",
        "\n",
        "[전체 코드 바로가기](practice/최지현_sts.ipynb)"
      ],
      "metadata": {
        "id": "BnR_ThZ6DUF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 클래스\n",
        "class CustomSTS(nn.Module):\n",
        "    def __init__(self, hidden_size: int, model_name):\n",
        "        super(CustomSTS, self).__init__()\n",
        "        self.bert_config = BertConfig.from_pretrained(model_name)   \n",
        "        self.model = BertForNextSentencePrediction.from_pretrained(model_name, config=self.bert_config)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
        "        \"\"\"\n",
        "        outputs(NextSentencePredictorOutput) : logtis, loss(next_sentence_label이 주어질 때 return)\n",
        "                                              hidden_states(optional), attentions(optional) 을 가지고 있다.\n",
        "        loss는 주어진 label이 0~5 사이의 값으로 scale 되어있기 때문에 직접 구해야한다!\n",
        "        \"\"\"\n",
        "        # logits's shape : (batch_size, 2)\n",
        "        logits = self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "        ).logits\n",
        "        probs = self.softmax(logits)\n",
        "        probs = probs[:, 0] * 5    # 0~5 사이의 값으로 정답(T)일 확률 뽑아내기\n",
        "        return probs    # 정답(T)일 확률, 정답일때 1 "
      ],
      "metadata": {
        "id": "wwBWkGbZWXCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [2] RoBERTModel을 받아 2 layer 추가(STS)\n",
        "\n",
        "[전체 코드 바로가기](practice/최지현_sts_roberta.ipynb)"
      ],
      "metadata": {
        "id": "gCx4yH8SDLzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomBertOnlyNSPHead(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
        "\n",
        "    def forward(self, pooled_output):\n",
        "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
        "        return seq_relationship_score\n",
        "# 모델 클래스\n",
        "class CustomSTS(nn.Module):\n",
        "    def __init__(self, model_name):\n",
        "        super(CustomSTS, self).__init__()\n",
        "        self.config = RobertaConfig.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name)\n",
        "        self.cls = CustomBertOnlyNSPHead(self.config)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.classifier = nn.Sequential(\n",
        "          nn.Linear(768, 32),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(0.1),\n",
        "          nn.Linear(32, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
        "        # logits's shape : (batch_size, seq_len, 2)\n",
        "        outputs = self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "        )['last_hidden_state']\n",
        "        logits = self.classifier(outputs)[:, 0, :]\n",
        "        probs = self.softmax(logits)\n",
        "        # probs = probs[:, 0] * 5    # 0~5 사이의 값으로 정답(T)일 확률 뽑아내기\n",
        "        return probs    # 정답(T)일 확률"
      ],
      "metadata": {
        "id": "5DAITTYmDLL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [3] AutoModelForSequenceClassification를 통해 분류\n",
        "\n",
        "[전체 코드 바로가기](practice/우경화_sts.ipynb)"
      ],
      "metadata": {
        "id": "LIg6Z8PiDYC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_labels = 1\n",
        "\n",
        "# model : klue/roberta-base 사용\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
        "\n",
        "# ...\n",
        "model_trainer.train()"
      ],
      "metadata": {
        "id": "vFA7_MJ2DZoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [4] SentenceTransformer와 EmbeddingSimilarityEvaluator을 통해 분류\n",
        "\n",
        "[전체 코드 바로가기](practice/김연식_sts.ipynb)"
      ],
      "metadata": {
        "id": "DBWhT61FDaB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "train_loss = losses.CosineSimilarityLoss(model=model)\n",
        "\n",
        "model = SentenceTransformer(modules=[embedding_model, pooler])\n",
        "evaluator = EmbeddingSimilarityEvaluator.from_input_examples(\n",
        "    test_examples_dt,\n",
        "    name=\"sts-dev\",\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    train_objectives=[(train_dataloader, train_loss)],\n",
        "    evaluator=evaluator,\n",
        "    epochs=num_epochs,\n",
        "    evaluation_steps=1000,\n",
        "    warmup_steps=warmup_steps,\n",
        "    output_path=model_save_path,\n",
        ")\n",
        "# test\n",
        "model = SentenceTransformer(model_save_path)\n",
        "test_evaluator(model, output_path=model_save_path)"
      ],
      "metadata": {
        "id": "SllPakOmDbFZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}